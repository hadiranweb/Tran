import streamlit as st
from openrouter import OpenRouter
import fitz  # PyMuPDF
import time
import os
from dotenv import load_dotenv
from fpdf import FPDF
import concurrent.futures
from tenacity import retry, stop_after_attempt, wait_exponential

# --- ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§ÙˆÙ„ÛŒÙ‡ ---
load_dotenv()
st.set_page_config(page_title="Ù…ØªØ±Ø¬Ù… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ", layout="wide")
st.title("ğŸ“– Ù…ØªØ±Ø¬Ù… Ù¾ÛŒØ´Ø±ÙØªÙ‡ PDF Ùˆ Ø²ÛŒØ±Ù†ÙˆÛŒØ³")

# Ø±Ø§Ø³Øªâ€ŒÚ†ÛŒÙ† Ú©Ø±Ø¯Ù† Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
st.markdown("""
<style>
@font-face {
    font-family: 'Vazir';
    src: url('fonts/Vazirmatn-Regular.ttf') format('truetype');
}

body, p, div, h1, h2, h3, h4, h5, h6 {
    font-family: 'Vazir', 'B Nazanin', Tahoma, sans-serif !important;
    direction: rtl;
    text-align: right;
}
</style>
""", unsafe_allow_html=True)

# --- ØªÙˆØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ ---
def process_pdf_by_page(uploaded_file):
    """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† ØµÙØ­Ù‡â€ŒØ¨Ù‡â€ŒØµÙØ­Ù‡ Ø§Ø² PDF"""
    uploaded_file.seek(0)
    doc = fitz.open(stream=uploaded_file.read(), filetype="pdf")
    return [page.get_text() for page in doc]

def process_srt_file(uploaded_file):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø²ÛŒØ±Ù†ÙˆÛŒØ³"""
    content = uploaded_file.read().decode("utf-8")
    blocks = content.split('\n\n')
    return [block.split('\n')[2] for block in blocks if len(block.split('\n')) >= 3]

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def translate_text_chunk(text, model_choice, delay=1):
    """ØªØ±Ø¬Ù…Ù‡ Ù‡Ø± Ø¨Ø®Ø´ Ù…ØªÙ† Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ùˆ Ù‚Ø§Ø¨Ù„ÛŒØª ØªÙ„Ø§Ø´ Ù…Ø¬Ø¯Ø¯"""
    time.sleep(delay)  # Ù…Ø¯ÛŒØ±ÛŒØª Rate Limit
    
    try:
        client = OpenRouter(api_key=os.getenv("OPENROUTER_API_KEY"))
        
        if model_choice == "DeepSeek":
            model_name = "deepseek-ai/deepseek-chat"
            system_message = "ØªÙˆ ÛŒÚ© Ù…ØªØ±Ø¬Ù… Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ Ù‡Ø³ØªÛŒ"
        else:
            model_name = "openai/gpt-3.5-turbo"
            system_message = "Ù…ØªØ±Ø¬Ù… Ø­Ø±ÙÙ‡â€ŒØ§ÛŒ ÙØ§Ø±Ø³ÛŒ"
        
        prompt = f"Ù…ØªÙ† Ø²ÛŒØ± Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ Ø±ÙˆØ§Ù† ØªØ±Ø¬Ù…Ù‡ Ú©Ù†:\n{text}"
        
        response = client.chat.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_message},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content
        
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {str(e)}")
        return f"[Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø§ÛŒÙ† Ø¨Ø®Ø´: {str(e)}]"

def parallel_translate(chunks, model_choice, max_workers=4):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ø¨Ø§ Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§ Ùˆ ØªØ§Ø®ÛŒØ±Ù‡Ø§ÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯"""
    translated_chunks = [None] * len(chunks)
    delay_per_worker = 2  # ØªØ§Ø®ÛŒØ± Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± worker
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        future_to_index = {}
        for i, chunk in enumerate(chunks):
            delay = (i % max_workers) * delay_per_worker
            future = executor.submit(
                translate_text_chunk, 
                chunk, 
                model_choice,
                delay
            )
            future_to_index[future] = i
        
        for future in concurrent.futures.as_completed(future_to_index):
            index = future_to_index[future]
            try:
                translated_chunks[index] = future.result()
                progress = (sum(1 for x in translated_chunks if x is not None)) / len(chunks)
                st.session_state['progress'] = progress
            except Exception as e:
                translated_chunks[index] = f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {str(e)}"
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡ Ø¨Ø®Ø´ {index + 1}: {str(e)}")
    
    return translated_chunks

def create_pdf(text, filename="ØªØ±Ø¬Ù…Ù‡.pdf"):
    pdf = FPDF()
    pdf.add_page()
    
    try:
        font_path = os.path.join(os.path.dirname(__file__), 'fonts')
        pdf.add_font('Vazir', '', os.path.join(font_path, 'Vazirmatn-Regular.ttf'), uni=True)
        pdf.set_font('Vazir', size=12)
    except:
        pdf.set_font("helvetica", size=12)
    
    pdf.multi_cell(0, 10, txt=text, align="R")
    return pdf.output(dest='S').encode('latin1')

# --- Ø±Ø§Ø¨Ø· Ú©Ø§Ø±Ø¨Ø±ÛŒ ---
with st.sidebar:
    st.header("ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒØ´Ø±ÙØªÙ‡")
    file_type = st.radio(
        "Ù†ÙˆØ¹ ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ:",
        ["PDF", "Ø²ÛŒØ±Ù†ÙˆÛŒØ³ (SRT)", "Ù…ØªÙ† Ø³Ø§Ø¯Ù‡"]
    )
    
    model_choice = st.radio(
        "Ù…Ø¯Ù„ ØªØ±Ø¬Ù…Ù‡:",
        ["DeepSeek", "OpenAI"]
    )
    
    parallel_enabled = st.checkbox("ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ÙˆØ§Ø²ÛŒ", value=True)
    max_workers = st.slider("ØªØ¹Ø¯Ø§Ø¯ ThreadÙ‡Ø§", 1, 8, 4) if parallel_enabled else 1

uploaded_file = st.file_uploader(
    "ÙØ§ÛŒÙ„ Ø®ÙˆØ¯ Ø±Ø§ Ø¢Ù¾Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯", 
    type=["pdf", "txt", "srt"],
    help="PDF, ÙØ§ÛŒÙ„ Ù…ØªÙ†ÛŒ ÛŒØ§ Ø²ÛŒØ±Ù†ÙˆÛŒØ³ SRT"
)

if uploaded_file:
    st.success(f"âœ… ÙØ§ÛŒÙ„ {uploaded_file.name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¢Ù¾Ù„ÙˆØ¯ Ø´Ø¯!")
    
    with st.expander("Ù†Ù…Ø§ÛŒØ´ Ù…Ø­ØªÙˆØ§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡"):
        if file_type == "PDF":
            chunks = process_pdf_by_page(uploaded_file)
            for i, chunk in enumerate(chunks[:3]):
                st.text(f"ØµÙØ­Ù‡ {i+1}:\n{chunk[:500]}...")
        elif file_type == "Ø²ÛŒØ±Ù†ÙˆÛŒØ³ (SRT)":
            chunks = process_srt_file(uploaded_file)
            st.text(f"ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ·: {len(chunks)}")
            st.text("\n".join(chunks[:5]))
        else:
            chunks = [uploaded_file.read().decode("utf-8")]
            st.text(chunks[0][:1000] + "...")
    
    st.info(f"ğŸ” {len(chunks)} Ø¨Ø®Ø´ Ù‚Ø§Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯")

    if st.button("Ø´Ø±ÙˆØ¹ ØªØ±Ø¬Ù…Ù‡", type="primary"):
        progress_bar = st.progress(0)
        status_text = st.empty()
        result_area = st.empty()
        
        start_time = time.time()
        
        if parallel_enabled and len(chunks) > 1:
            translated = parallel_translate(chunks, model_choice, max_workers)
        else:
            translated = []
            for i, chunk in enumerate(chunks):
                progress = (i + 1) / len(chunks)
                progress_bar.progress(progress)
                status_text.text(f"Ø¯Ø± Ø­Ø§Ù„ ØªØ±Ø¬Ù…Ù‡ Ø¨Ø®Ø´ {i + 1}/{len(chunks)}...")
                translated.append(translate_text_chunk(chunk, model_choice))
        
        full_translation = "\n\n".join(translated)
        result_area.text_area("ØªØ±Ø¬Ù…Ù‡ Ú©Ø§Ù…Ù„", full_translation, height=400)
        
        elapsed_time = time.time() - start_time
        st.success(f"â±ï¸ ØªØ±Ø¬Ù…Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯! Ø²Ù…Ø§Ù† Ù¾Ø±Ø¯Ø§Ø²Ø´: {elapsed_time:.2f} Ø«Ø§Ù†ÛŒÙ‡")
        
        col1, col2 = st.columns(2)
        with col1:
            st.download_button(
                "Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØ±Ø¬Ù…Ù‡ (PDF)",
                data=create_pdf(full_translation),
                file_name=f"ØªØ±Ø¬Ù…Ù‡_{uploaded_file.name.split('.')[0]}.pdf",
                mime="application/pdf"
            )
        with col2:
            st.download_button(
                "Ø¯Ø§Ù†Ù„ÙˆØ¯ ØªØ±Ø¬Ù…Ù‡ (TXT)",
                data=full_translation,
                file_name=f"ØªØ±Ø¬Ù…Ù‡_{uploaded_file.name.split('.')[0]}.txt",
                mime="text/plain"
            )

        st.session_state['last_translation'] = full_translation
